{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer learning ResNet50 cifar-10.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOXHTWc8u5Sp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Jun 15 06:27:00 2019\n",
        "\n",
        "@author: shubhamtripathi\n",
        "\"\"\"\n",
        "#%%\n",
        "from keras.applications import resnet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.models import load_model, Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#%% flags\n",
        "flag_save_data  = True\n",
        "flag_load_model = True\n",
        "\n",
        "#%% read cifar-10 dataset from directory\n",
        "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict_ = pickle.load(fo, encoding='bytes')\n",
        "    return dict_\n",
        "#batch_1 = unpickle('cifar-10-batches-py/data_batch_1')\n",
        "# read first 10 entries of batch_1\n",
        "#for (key,val) in batch_1.items():\n",
        "#    print('{0}:{1}'.format(key,val[:2]))\n",
        "\n",
        "#%% save the data as image in correct directory hierarchy\n",
        "def save_data(data_dir, sub_data_dir, batch):\n",
        "    sub_data_path = os.path.join(data_dir,sub_data_dir)\n",
        "    if not os.path.isdir(sub_data_path):\n",
        "        os.makedirs(sub_data_path)\n",
        "    \n",
        "    for j,img in enumerate(batch[b'data']):\n",
        "        img1 = []\n",
        "        base = 1024\n",
        "        for i in range(base):\n",
        "            img1.extend([img[i],img[i+base],img[i+base*2]])\n",
        "        img2 = np.resize(img1, (32,32,3))\n",
        "        # show image\n",
        "        # plt.imshow(img2)\n",
        "        class_dir = str(batch[b'labels'][j])\n",
        "        class_path = os.path.join(data_dir,sub_data_dir,class_dir)\n",
        "        if not os.path.isdir(class_path):\n",
        "            os.makedirs(class_path)\n",
        "        \n",
        "        img_name = batch[b'filenames'][j].decode('utf-8')\n",
        "        file_path = os.path.join(data_dir, sub_data_dir, class_dir, img_name)\n",
        "        img3 = Image.fromarray(img2)\n",
        "        img3.save(file_path)\n",
        "\n",
        "#%%  load training, validation and testing data and save in directories\n",
        "read_dir = 'cifar-10-batches-py'\n",
        "data_dir = 'cifar-10-data'\n",
        "if flag_save_data:\n",
        "    for i in range(1,7):\n",
        "        sub_data_dir = 'train'\n",
        "        batch_file_name = 'data_batch_' + str(i)\n",
        "        if i == 5:\n",
        "            sub_data_dir = 'valid'\n",
        "        if i == 6:\n",
        "            sub_data_dir = 'test'\n",
        "            batch_file_name = 'test_batch'\n",
        "        batch = unpickle(os.path.join(read_dir, batch_file_name))\n",
        "        save_data(data_dir, sub_data_dir, batch)\n",
        "\n",
        "\n",
        "#%% create image data generator\n",
        "img_data_generator = ImageDataGenerator(rescale = 1./255, \n",
        "                                        preprocessing_function=preprocess_input)\n",
        "train_dir = 'train'\n",
        "valid_dir = 'valid'\n",
        "test_dir  = 'test'\n",
        "train_path = os.path.join(data_dir,train_dir)\n",
        "valid_path = os.path.join(data_dir,valid_dir)\n",
        "test_path = os.path.join(data_dir,test_dir)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_gen = img_data_generator.flow_from_directory(train_path, \n",
        "                                                   target_size=(200,200), \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   class_mode='categorical')\n",
        "valid_gen = img_data_generator.flow_from_directory(valid_path, \n",
        "                                                   target_size=(200,200), \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   class_mode='categorical')\n",
        "test_gen = img_data_generator.flow_from_directory(test_path, \n",
        "                                                   target_size=(200,200), \n",
        "                                                   batch_size=batch_size, \n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "#%% load ResNet50 saved model\n",
        "\n",
        "#%% fetch model from internet\n",
        "if flag_load_model:\n",
        "    base_model_file_name = 'ResNet50_model_features.h5'\n",
        "    if not os.path.exists(base_model_file_name):\n",
        "        base_model = resnet50.ResNet50(input_shape=(200,200,3), weights='imagenet',include_top=False)\n",
        "        # save model locally\n",
        "        base_model.save(base_model_file_name)\n",
        "    else:\n",
        "        base_model = load_model(base_model_file_name)\n",
        "\n",
        "##%% delete model\n",
        "#del base_model\n",
        "\n",
        "#%% add fully connected layers\n",
        "# source: https://keras.io/applications/\n",
        "x = base_model.output\n",
        "#x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "#%%\n",
        "num_train_samples = train_gen.n\n",
        "num_valid_samples = valid_gen.n\n",
        "factor = 5\n",
        "checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)\n",
        "model.fit_generator(train_gen, \n",
        "                    steps_per_epoch=num_train_samples//(batch_size*factor), \n",
        "                    epochs=1, \n",
        "                    shuffle=True, \n",
        "                    callbacks=[checkpointer],\n",
        "                    validation_data=valid_gen,\n",
        "                    validation_steps=num_valid_samples//(batch_size*factor))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}