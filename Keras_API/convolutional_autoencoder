#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jun 19 06:19:56 2019

@author: shubhamtripathi
"""

# Autoencoder
# https://blog.keras.io/building-autoencoders-in-keras.html

#%% import libraries
from keras.layers import Conv2D, Input, Dense, MaxPooling2D, UpSampling2D
from keras.models import Model
from keras import backend as K
from keras.datasets import mnist
from keras.callbacks import TensorBoard
#from keras.callbacks import ModelCheckpoint

import numpy as np
import matplotlib.pyplot as plt

# %% convolutional autoencoder

input_img = Input(shape=(28,28,1))
x = Conv2D(16, (3,3), activation='relu', padding='same')(input_img)
x = MaxPooling2D((2,2), padding='same')(x)
x = Conv2D(8, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2), padding='same')(x)
x = Conv2D(8, (3,3), activation='relu', padding='same')(x)
encoded = MaxPooling2D((2,2), padding='same')(x)

x = Conv2D(8, (3,3), activation='relu', padding='same')(encoded)
x = UpSampling2D((2,2))(x)
x = Conv2D(8, (3,3), activation='relu', padding='same')(x)
x = UpSampling2D((2,2))(x)
x = Conv2D(8, (3,3), activation='relu')(x)
x = UpSampling2D((2,2))(x)
decoded = Conv2D(1, (3,3), activation='relu', padding='same')(x)

autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])

#%% import dataset mnist
((x_train, _), (x_test, _)) = mnist.load_data() # load data
x_train = x_train.astype('float32') / 255.
x_test  = x_test.astype('float32') / 255.
x_train = np.reshape(x_train,(len(x_train),28,28,1))
x_test  = np.reshape(x_test, (len(x_test),28,28,1))

#%% train model and visualize loss/accuracy in tensorboard at http://0.0.0.0:6006
#checkpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, save_best_only=True)
autoencoder.fit(x_train, x_train, 
                epochs=10, 
                batch_size=128, 
                shuffle=True,
                validation_data=(x_test,x_test),
                callbacks=[TensorBoard(log_dir='/tmp/autoencoder')])


"""
Train on 60000 samples, validate on 10000 samples
Epoch 1/10
60000/60000 [==============================] - 35s 584us/step - loss: 0.2426 - acc: 0.7953 - val_loss: 0.2156 - val_acc: 0.7771
Epoch 2/10
60000/60000 [==============================] - 35s 579us/step - loss: 0.1930 - acc: 0.7949 - val_loss: 0.1863 - val_acc: 0.7960
Epoch 3/10
60000/60000 [==============================] - 36s 593us/step - loss: 0.1853 - acc: 0.7958 - val_loss: 0.1859 - val_acc: 0.8024
Epoch 4/10
60000/60000 [==============================] - 35s 587us/step - loss: 0.1762 - acc: 0.7979 - val_loss: 0.1688 - val_acc: 0.7988
Epoch 5/10
60000/60000 [==============================] - 37s 619us/step - loss: 0.1692 - acc: 0.7995 - val_loss: 0.1642 - val_acc: 0.8012
Epoch 6/10
60000/60000 [==============================] - 37s 612us/step - loss: 0.1673 - acc: 0.7996 - val_loss: 0.1609 - val_acc: 0.8018
Epoch 7/10
60000/60000 [==============================] - 38s 627us/step - loss: 0.1623 - acc: 0.8014 - val_loss: 0.1551 - val_acc: 0.7998
Epoch 8/10
60000/60000 [==============================] - 37s 622us/step - loss: 0.1609 - acc: 0.8017 - val_loss: 0.1532 - val_acc: 0.8012
Epoch 9/10
60000/60000 [==============================] - 36s 604us/step - loss: 0.1577 - acc: 0.8022 - val_loss: 0.2110 - val_acc: 0.8089
Epoch 10/10
60000/60000 [==============================] - 35s 583us/step - loss: 0.1558 - acc: 0.8022 - val_loss: 0.1554 - val_acc: 0.7971
"""


#%% predict images
decoded_imgs = autoencoder.predict(x_test)

#%% visualize images
n = 10
plt.figure(figsize=(20,4))
for i in range(1,n+1):
    ax = plt.subplot(2, n, i)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
    
    ax = plt.subplot(2, n, n+i)
    plt.imshow(decoded_imgs[i].reshape(28,28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()












